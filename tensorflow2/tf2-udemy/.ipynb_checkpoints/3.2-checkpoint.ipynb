{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load in the data(X and Y)\n",
    "2. Instantiate the model\n",
    "3. Train (\"fit\") the model\n",
    "4. Evaulate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Architecture of the model (equation to go from input to prediction)\n",
    "2. How is the model trained?\n",
    "    * Cost / loss / error function\n",
    "    * Gradient descent to minimize the cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Decision Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y = mx + b\n",
    "\n",
    "x2 = mx1 + b\n",
    "\n",
    "w1x1 + w2x2 + b = 0\n",
    "\n",
    "\n",
    "a = w1x1 + w2x2 + b\n",
    "\n",
    "if a >= 0 -> predict 1\n",
    "\n",
    "if a < 0 -> predict 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/3-2-1.png)\n",
    "![title](images/3-2-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y^ = u(a), a = w1x1 + w2x2 + b\n",
    "\n",
    "with this function\n",
    "\n",
    "we predict\n",
    "\n",
    "1 if a >= 0\n",
    "\n",
    "0 if a < 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y^ = ro(a), a = w1x1 + w2x2 + b (**sigmoid** function, smoother than previous function)\n",
    "\n",
    "but if we use sigmoid function\n",
    "\n",
    "then we predict with following rule\n",
    "\n",
    "p(y=1|x) = ro(w1x1 + w2x2 + b)\n",
    "\n",
    "if p(y=1|x) >= 50% -> predict 1, else 0\n",
    "\n",
    "check the following graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/3-2-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The \"S-shaped\" curve is called a sigmoid\n",
    "* This model is called logistic **regression**\n",
    "* sigmoid = logistic function\n",
    "* Its argument is called the \"logit\", although a more modern name is **\"activation\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with > 2 Inputs\n",
    "\n",
    "p(y = 1|x) = ro(W<sup>T</sup>X+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Tensorflow 2.0\n",
    "* Is mostly an exercise in \"API hunting\"\n",
    "* The actual math isn't written in code(by you)\n",
    "* Your job is to find the function that implements the math in question\n",
    "* We want: <code>tf.keras.layers.Dense(output_size)</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=(D,)),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "   ])\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost\n",
    "\n",
    "<code>model.compile(optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy'])</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all predictions are correct then cost function will return 0\n",
    "\n",
    "if all predictions are incorrect then cost function will return larger values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/3-2-4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for decrease cost\n",
    "\n",
    "* The general strategy is gradient descent\n",
    "* But there are many flavors of gradient descent\n",
    "* The typical default in deep learning is \"adam\"\n",
    "\n",
    "<code>model.compile(**optimizer='adam'**,\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy'])</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "\n",
    "accuracy = # of correct / # of total\n",
    "\n",
    "<code>model.compile(optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    **metrics=['accuracy']**)</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training / Fitting\n",
    "\n",
    "<code>r = model.fit(X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=100)</code>\n",
    "    \n",
    "This basically just does following graph in a loop\n",
    "\n",
    "![title](images/4-4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>r = model.fit(X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=100)\n",
    "plt.plot(**r.history['loss']**, label='loss')\n",
    "plt.plot(**r.history['val_loss']**, label='val_loss')</code>\n",
    "\n",
    "![title](images/3-2-5.png)\n",
    "\n",
    "we can use this data to determine how many epochs we need "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "* Prediction = Round(ro(W<sup>T</sup>x + b))\n",
    "* Implemented with Keras Dense layer\n",
    "* compile() to specify optmizer(adam), loss(binary cross entropy), metrics (accuracy)\n",
    "* Note: we're using binary cross entropy because there will be 2 labels in our dataset\n",
    "* fit() returns a history object, so we can plot loss per iteration"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
